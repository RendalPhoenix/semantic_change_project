{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  \n",
    "import pandas as pd  \n",
    "from time import time  \n",
    "from collections import defaultdict  \n",
    "import numpy as np\n",
    "import logging  \n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "from scipy.spatial.distance import cosine\n",
    "basecsvpath=\"data/w2v_models\"\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years=['2008','2016','2020']\n",
    "model_2008 = Word2Vec.load(basecsvpath+\"_\"+years[0]+\"_word2vec_full.model\")\n",
    "model_2016 = Word2Vec.load(basecsvpath+\"_\"+years[1]+\"_word2vec_full.model\")\n",
    "model_2020 = Word2Vec.load(basecsvpath+\"_\"+years[2]+\"_word2vec_full.model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Procrustes alignment for the models\n",
    "code from https://gist.github.com/quadrismegistus/09a93e219a6ffc4f216fb85235535faf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_procrustes_align_gensim(base_embed, other_embed, words=None):\n",
    "    \"\"\"\n",
    "    Original script: https://gist.github.com/quadrismegistus/09a93e219a6ffc4f216fb85235535faf\n",
    "    Procrustes align two gensim word2vec models (to allow for comparison between same word across models).\n",
    "    Code ported from HistWords <https://github.com/williamleif/histwords> by William Hamilton <wleif@stanford.edu>.\n",
    "        \n",
    "    First, intersect the vocabularies (see `intersection_align_gensim` documentation).\n",
    "    Then do the alignment on the other_embed model.\n",
    "    Replace the other_embed model's syn0 and syn0norm numpy matrices with the aligned version.\n",
    "    Return other_embed.\n",
    "    If `words` is set, intersect the two models' vocabulary with the vocabulary in words (see `intersection_align_gensim` documentation).\n",
    "    \"\"\"\n",
    "\n",
    "    # patch by Richard So [https://twitter.com/richardjeanso) (thanks!) to update this code for new version of gensim\n",
    "    # base_embed.init_sims(replace=True)\n",
    "    # other_embed.init_sims(replace=True)\n",
    "\n",
    "    # make sure vocabulary and indices are aligned\n",
    "    in_base_embed, in_other_embed = intersection_align_gensim(base_embed, other_embed, words=words)\n",
    "    # print(in_base_embed.wv.shape)\n",
    "    # print(in_other_embed.wv.shape)\n",
    "    # get the (normalized) embedding matrices\n",
    "\n",
    "    in_base_embed.wv.fill_norms(force=True)\n",
    "    in_other_embed.wv.fill_norms(force=True)\n",
    "\n",
    "    base_vecs = in_base_embed.wv.get_normed_vectors()\n",
    "    other_vecs = in_other_embed.wv.get_normed_vectors()\n",
    "\n",
    "    # just a matrix dot product with numpy\n",
    "    m = other_vecs.T.dot(base_vecs) \n",
    "    # SVD method from numpy\n",
    "    u, _, v = np.linalg.svd(m)\n",
    "    # another matrix operation\n",
    "    ortho = u.dot(v) \n",
    "    # Replace original array with modified one, i.e. multiplying the embedding matrix by \"ortho\"\n",
    "    other_embed.wv.vectors = (other_embed.wv.vectors).dot(ortho)    \n",
    "    \n",
    "    return other_embed\n",
    "\n",
    "def intersection_align_gensim(m1, m2, words=None):\n",
    "    \"\"\"\n",
    "    Intersect two gensim word2vec models, m1 and m2.\n",
    "    Only the shared vocabulary between them is kept.\n",
    "    If 'words' is set (as list or set), then the vocabulary is intersected with this list as well.\n",
    "    Indices are re-organized from 0..N in order of descending frequency (=sum of counts from both m1 and m2).\n",
    "    These indices correspond to the new syn0 and syn0norm objects in both gensim models:\n",
    "        -- so that Row 0 of m1.syn0 will be for the same word as Row 0 of m2.syn0\n",
    "        -- you can find the index of any word on the .index2word list: model.index2word.index(word) => 2\n",
    "    The .vocab dictionary is also updated for each model, preserving the count but updating the index.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the vocab for each model\n",
    "    vocab_m1 = set(m1.wv.index_to_key)\n",
    "    vocab_m2 = set(m2.wv.index_to_key)\n",
    "\n",
    "    # Find the common vocabulary\n",
    "    common_vocab = vocab_m1 & vocab_m2\n",
    "    if words: common_vocab &= set(words)\n",
    "\n",
    "    # If no alignment necessary because vocab is identical...\n",
    "    if not vocab_m1 - common_vocab and not vocab_m2 - common_vocab:\n",
    "        return (m1,m2)\n",
    "\n",
    "    # Otherwise sort by frequency (summed for both)\n",
    "    common_vocab = list(common_vocab)\n",
    "    common_vocab.sort(key=lambda w: m1.wv.get_vecattr(w, \"count\") + m2.wv.get_vecattr(w, \"count\"), reverse=True)\n",
    "    print(len(common_vocab))\n",
    "\n",
    "    # Then for each model...\n",
    "    for m in [m1, m2]:\n",
    "        # Replace old syn0norm array with new one (with common vocab)\n",
    "        indices = [m.wv.key_to_index[w] for w in common_vocab]\n",
    "        old_arr = m.wv.vectors\n",
    "        new_arr = np.array([old_arr[index] for index in indices])\n",
    "        m.wv.vectors = new_arr\n",
    "\n",
    "        # Replace old vocab dictionary with new one (with common vocab)\n",
    "        # and old index2word with new one\n",
    "        new_key_to_index = {}\n",
    "        new_index_to_key = []\n",
    "        for new_index, key in enumerate(common_vocab):\n",
    "            new_key_to_index[key] = new_index\n",
    "            new_index_to_key.append(key)\n",
    "        m.wv.key_to_index = new_key_to_index\n",
    "        m.wv.index_to_key = new_index_to_key\n",
    "        \n",
    "        print(len(m.wv.key_to_index), len(m.wv.vectors))\n",
    "        \n",
    "    return (m1,m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2020_aligned2_08=smart_procrustes_align_gensim(model_2008,model_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get most frequent 2000 words and calculate cosine dist across time\n",
    "vb_2020=set(model_2008.wv.index_to_key[:2000])\n",
    "vb_list=list(vb_2020)\n",
    "vb_sim={\"word\":[],\"cos_dist_08_20\":[]}\n",
    "for word in vb_list:\n",
    "    vb_sim[\"word\"].append(word)\n",
    "    word_2020=model_2020_aligned2_08.wv[word]\n",
    "    word_2008=model_2008.wv[word]    \n",
    "    word_sim=1-cosine(word_2020,word_2008)\n",
    "    vb_sim[\"cos_dist_08_20\"].append(word_sim)\n",
    "\n",
    "df_vb_sim=pd.DataFrame(vb_sim)\n",
    "sorted_df=df_vb_sim.sort_values(by=['cos_dist_08_20'],ascending=False)\n",
    "word_list=sorted_df['word'].to_list()\n",
    "#save to local file\n",
    "# sorted_df.to_excel(basecsvpath+\"_most_feq_2000.xls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "similar_words_list_kw=[]\n",
    "# keywords=['loss','train','solution','performance','attention','feature','training','learning','language']\n",
    "keywords=['slack','loss','spark','train','training','react','language','performance','learning','attention','feature','table','value','file']\n",
    "\n",
    "for kw in keywords:\n",
    "    similar_words_list_year=[]\n",
    "    kw_2020=model_2020_aligned2_08.wv[kw]\n",
    "    kw_2008=model_2008.wv[kw]\n",
    "    kw_similarity=1-cosine(kw_2020,kw_2008)\n",
    "    similar_words_list_year.append(kw_similarity)\n",
    "\n",
    "    similar_words_2008=model_2008.wv.similar_by_word(kw,topn=10)\n",
    "    similar_words_string_2008=\"\"\n",
    "    for word in similar_words_2008:\n",
    "        similar_words_string_2008+=word[0]+','\n",
    "    similar_words_list_year.append(similar_words_string_2008)\n",
    "\n",
    "\n",
    "    similar_words_2020=model_2020_aligned2_08.wv.similar_by_word(kw,topn=10)\n",
    "    similar_words_string_2020=\"\"\n",
    "    for word in similar_words_2020:\n",
    "        similar_words_string_2020+=word[0]+','\n",
    "    similar_words_list_year.append(similar_words_string_2020)\n",
    "    similar_words_list_kw.append(similar_words_list_year)\n",
    "\n",
    "    \n",
    "df = pd.DataFrame(similar_words_list_kw,\n",
    "                index=pd.Index(keywords),\n",
    "                columns=[\"cosine dist\",\"2008\",\"2020\"])\n",
    "srt_df=df.sort_values(by=['cosine dist'],ascending=False)\n",
    "#save to local file\n",
    "# srt_df.to_excel(basecsvpath+\"_08_20.xls\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot for PCA visualization\n",
    "\n",
    "the main func is a fork of the code here\n",
    "https://gist.github.com/marcellusruben/0be4e45eb342f664621166ed3c6e952f#file-pca_3d-py\n",
    "\n",
    "modify it to visualize word vectors from different time periods in 2d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_list(sim_words, words):\n",
    "    \n",
    "    list_of_words = []\n",
    "    \n",
    "    for i in range(len(sim_words)):\n",
    "        \n",
    "        sim_words_list = list(sim_words[i])\n",
    "        sim_words_list.append(words)\n",
    "        sim_words_tuple = tuple(sim_words_list)\n",
    "        list_of_words.append(sim_words_tuple)\n",
    "        \n",
    "    return list_of_words\n",
    "\n",
    "\n",
    "\n",
    "def pca_scatter(model_1,model_2, user_input=None, words=None, label=None, color_map=None, topn=10, sample=10):\n",
    "    \n",
    "\n",
    "    w1=words[0:10]\n",
    "    w2=words[10:20]\n",
    "    inputs=words[20:22]\n",
    "    word_vectors_1 = np.array([model_1[w] for w in w1])\n",
    "    word_vectors_2 = np.array([model_2[w] for w in w2])\n",
    "    word_vectors_3 = np.array([model_1[w.split(\"-\")[0]] for w in [inputs[0]]])\n",
    "    word_vectors_4 = np.array([model_2[w.split(\"-\")[0]] for w in [inputs[1]]])\n",
    "    word_vectors=np.concatenate((word_vectors_1,word_vectors_2,word_vectors_3,word_vectors_4))\n",
    "    two_dim = PCA(random_state=0).fit_transform(word_vectors)[:,:2]\n",
    "\n",
    "    data = []\n",
    "    count = 0\n",
    "    \n",
    "    for i in range (len(user_input)):\n",
    "\n",
    "                trace = go.Scatter(\n",
    "                    x = two_dim[count:count+topn,0], \n",
    "                    y = two_dim[count:count+topn,1],  \n",
    "                    text = words[count:count+topn],\n",
    "                    name = user_input[i],\n",
    "                    textposition = \"bottom center\",\n",
    "                    textfont_size = 20,\n",
    "                    mode = 'markers+text',\n",
    "                    marker = {\n",
    "                        'size': 10,\n",
    "                        'opacity': 0.8,\n",
    "                        'color': 2\n",
    "                    }\n",
    "       \n",
    "                )\n",
    "                \n",
    "            \n",
    "                data.append(trace)\n",
    "                count = count+topn\n",
    "\n",
    "    trace_input = go.Scatter(\n",
    "                    x = two_dim[count:,0], \n",
    "                    y = two_dim[count:,1],  \n",
    "                    text = words[count:],\n",
    "                    name = 'target word',\n",
    "                    textposition = \"bottom center\",\n",
    "                    textfont_size = 20,\n",
    "                    mode = 'markers+text',\n",
    "                    marker = {\n",
    "                        'size': 10,\n",
    "                        'opacity': 1,\n",
    "                        'color': 'black'\n",
    "                    }\n",
    "                    )\n",
    "           \n",
    "    data.append(trace_input)\n",
    "    \n",
    "\n",
    "    layout = go.Layout(\n",
    "        margin = {'l': 0, 'r': 0, 'b': 0, 't': 0},\n",
    "        showlegend=True,\n",
    "        legend=dict(\n",
    "        x=0.4,\n",
    "        y=-0.2,\n",
    "        font=dict(\n",
    "            family=\"Courier New\",\n",
    "            size=25,\n",
    "            color=\"black\"\n",
    "        )),\n",
    "        font = dict(\n",
    "            family = \" Courier New \",\n",
    "            size = 15),\n",
    "        autosize = False,\n",
    "        width = 1000,\n",
    "        height = 1300\n",
    "        )\n",
    "\n",
    "\n",
    "    plot_figure = go.Figure(data = data, layout = layout)\n",
    "    plot_figure.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_user_input = ['loss-2008','loss-2016']\n",
    "ml_result_word = []\n",
    "\n",
    "for words in ml_user_input:\n",
    "    if words.split(\"-\")[1]==\"2008\":\n",
    "        sim_words = model_2008.wv.most_similar(words.split(\"-\")[0], topn=10)\n",
    "        sim_words = append_list(sim_words, words)\n",
    "    else:\n",
    "        sim_words = model_2020_aligned2_08.wv.most_similar(words.split(\"-\")[0], topn=10)\n",
    "        sim_words = append_list(sim_words, words)\n",
    "        \n",
    "    ml_result_word.extend(sim_words)\n",
    "\n",
    "ml_similar_word = [word[0] for word in ml_result_word]\n",
    "ml_similar_word.extend(ml_user_input)\n",
    "labels = [word[2] for word in ml_result_word]\n",
    "label_dict = dict([(y,x+1) for x,y in enumerate(set(labels))])\n",
    "color_map = [label_dict[x] for x in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_scatter(model_2008.wv,model_2020_aligned2_08.wv, ml_user_input, ml_similar_word, color_map)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
